# -*- coding: utf-8 -*-
"""sql-olist-ecommerce-data-analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrlpBQjrChRwxsBIp9Zn6fdNxSsQaTIz

# An√°lise de Dados do E-commerce Olist com SQL e Python

Ol√°! meu nome √© Matheus Louren√ßo

Neste projeto, exploro como o SQL pode ser usado de forma eficiente para an√°lise de dados, enquanto o Python entra em cena apenas para visualiza√ß√£o e storytelling.

O conjunto de dados utilizado vem da Olist (https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), uma plataforma brasileira de e-commerce que conecta pequenos lojistas a grandes marketplaces. O dataset inclui mais de 99 mil pedidos realizados entre 2016 e 2018, oferecendo uma base rica para descobrir padr√µes de vendas, comportamento do cliente e efici√™ncia log√≠stica.

Os arquivos CSV originais foram importados para um banco de dados SQLite, onde realizei todo o processo de limpeza, transforma√ß√£o e an√°lise diretamente em SQL. Em seguida, usei Python para criar visualiza√ß√µes que traduzem os resultados em insights visuais e estrat√©gicos.

## Conectando ao Banco de Dados

O SQLite √© uma op√ß√£o bem pr√°tica.
Ele vai armazenar todo o banco em um √∫nico arquivo e n√£o precisa de servidor ativo em segundo plano.
Usando a biblioteca sqlite3, √© poss√≠vel acessar e consultar os dados diretamente pelo Python.
"""

import pandas as pd
import sqlite3
import os

# Conectar/criar o banco
conn = sqlite3.connect('/content/olist.sqlite')

# Lista de CSVs e nomes das tabelas
csv_files = {
    "customers": "olist_customers_dataset.csv",
    "geolocation": "olist_geolocation_dataset.csv",
    "order_items": "olist_order_items_dataset.csv",
    "order_payments": "olist_order_payments_dataset.csv",
    "order_reviews": "olist_order_reviews_dataset.csv",
    "orders": "olist_orders_dataset.csv",
    "products": "olist_products_dataset.csv",
    "sellers": "olist_sellers_dataset.csv",
    "category_translation": "product_category_name_translation.csv"
}

# Carregando cada CSV e salvar no banco SQLite
for table_name, file_name in csv_files.items():
    df = pd.read_csv(f"/content/{file_name}")
    df.to_sql(table_name, conn, if_exists='replace', index=False)
    print(f"Tabela '{table_name}' criada com {len(df)} registros.")

# Confirmando as tabelas criadas
tables = pd.read_sql_query("SELECT name FROM sqlite_master WHERE type='table';", conn)
print("\nTabelas dispon√≠veis no banco:")
print(tables)

"""Para consultar o banco de dados, basta escrever a query SQL como uma string e pass√°-la para o 'pandas.read_sql_query', junto com a conex√£o do banco.
O resultado √© retornado em um DataFrame do pandas, facilitando a an√°lise e visualiza√ß√£o dos dados.

Ao longo deste notebook, utilizaremos a seguinte fun√ß√£o para explorar as tabelas do banco de dados de forma simples e direta:
"""

import pandas as pd

def view_table(table, limit=5):
    query = f"""
        SELECT *
        FROM {table}
        LIMIT {limit}
    """
    return pd.read_sql_query(query, conn)

# visualizar as 5 primeiras linhas da tabela 'orders'
view_table('orders')

"""# N√∫mero de Pedidos
A tabela 'orders' re√∫ne os IDs de pedidos e clientes, al√©m do status de entrega, que pode ser ‚Äúdelivered‚Äù ou outros estados anteriores √† entrega.
"""

# Tabela de pedidos, mostrando as 3 primeiras colunas
view_table('orders', 5).iloc[:, :3]

"""As pr√≥ximas colunas mostram os hor√°rios das mudan√ßas de status do pedido e, por √∫ltimo, a data estimada de entrega"""

# resto das colunas
view_table('orders', 5).iloc[:, 3:]

# Contagem de pedidos por dia
orders_per_day = """
SELECT
    DATE(order_purchase_timestamp) AS day,
    COUNT(*) AS order_count
FROM orders
GROUP BY day
ORDER BY day
"""

df = pd.read_sql_query(orders_per_day, conn)
df.head(5)

"""No resultado anterior, podemos ver que os primeiros dias possuem poucos pedidos. Vamos usar o Matplotlib para visualizar a frequ√™ncia de pedidos por dia:"""

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Aumento do tamanho da fonte
plt.rcParams['font.size'] = 12
plt.rcParams['axes.titlesize'] = 16

# Plotando o gr√°fico de linha
plt.figure(figsize=(14, 6))
plt.plot(pd.to_datetime(df['day']), df['order_count'], color='green')
plt.ylabel('N√∫mero de pedidos')
plt.title('N√∫mero de pedidos por dia')
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
plt.xticks(rotation=45)
plt.show()

"""Pode-se perceber um pico de pedidos pr√≥ximo ao Natal, especialmente em 24 de dezembro. Tamb√©m h√° um crescimento constante nas vendas ao longo do tempo, refletindo a expans√£o da Olist. Como o in√≠cio e o fim do per√≠odo t√™m poucos dados, essas datas ser√£o ignoradas nas pr√≥ximas an√°lises.

Agora, vamos investigar quando os pedidos acontecem ‚Äî por dia da semana e hora do dia ‚Äî criando um mapa de calor que mostra o volume de pedidos ao longo da semana e das horas. Para isso, extrairemos o dia da semana e a hora do pedido usando a fun√ß√£o STRFTIME.
"""

order_day_hour = """
SELECT
    -- dia da semana abreviado
    CASE STRFTIME('%w', order_purchase_timestamp)
        WHEN '1' THEN 'Mon'
        WHEN '2' THEN 'Tue'
        WHEN '3' THEN 'Wed'
        WHEN '4' THEN 'Thu'
        WHEN '5' THEN 'Fri'
        WHEN '6' THEN 'Sat'
        WHEN '0' THEN 'Sun'
        END AS day_of_week_name,
    -- dia da semana como um inteiro (Sunday=7)
    CAST(STRFTIME('%w', order_purchase_timestamp) AS INTEGER) AS day_of_week_int,
    -- Hour of the day (0-24)
    CAST(STRFTIME("%H", order_purchase_timestamp) AS INTEGER) AS hour
FROM orders
"""

pd.read_sql_query(order_day_hour, conn)

"""Usamos uma 'list comprehension' em Python para gerar automaticamente a contagem de pedidos por hora, sem precisar escrever 24 linhas manuais. A consulta anterior √© reaproveitada como uma subconsulta tempor√°ria para facilitar a an√°lise."""

count_orders_per_hour = ',\n    '.join([
    f'COUNT(CASE WHEN hour = {i} THEN 1 END) AS "{i}"' \
    for i in range(24)
])

orders_per_day_of_the_week_and_hour = f"""
WITH OrderDayHour AS (
    {order_day_hour}
)
SELECT
    day_of_week_name,
    {count_orders_per_hour}
FROM OrderDayHour
GROUP BY day_of_week_int
ORDER BY day_of_week_int
"""

# SQL query sem uma subconsulta tempor√°ria
print(orders_per_day_of_the_week_and_hour[591:])

"""Depois de executar a consulta, precisamos apenas definir a coluna 'day_of_the_week_name' como o √≠ndice do dataframe para obter a matriz necess√°ria para construir o mapa de calor:"""

df = pd.read_sql_query(orders_per_day_of_the_week_and_hour, conn)
df = df.set_index('day_of_week_name')
df

"""Podemos usar a biblioteca do python 'seaborn' para criar o mapa de calor:"""

import seaborn as sns

fig, ax = plt.subplots(figsize=(14, 6))
sns.heatmap(df, cmap='YlGnBu', cbar=False)
media_orders = df.mean().mean()
for i in range(len(df)):
    for j in range(len(df.columns)):
        text_color = 'white' if df.iloc[i, j] > media_orders else 'black'
        ax.text(j+0.5, i+0.5, int(df.iloc[i, j]),
            color=text_color, fontsize=10, ha="center", va="center")
plt.title("N√∫mero de dias por dia da semana e pela hora do dia")
plt.xlabel("Hora do dia")
plt.ylabel("")
plt.show()

"""Pode-se observar que a maioria dos pedidos foi feita em dias de semana, entre *10h e 16h*, com uma leve queda por volta do meio-dia (hor√°rio de almo√ßo). H√° tamb√©m um aumento de pedidos √† noite, por volta das 21h, de domingo a quinta-feira.

O s√°bado √© o dia com menos pedidos, embora ainda movimentado.
J√° o hor√°rio com menor atividade √© entre 3h e 5h da manh√£, mas ainda ocorrem algumas compras nesse per√≠odo

Agora que analisamos a distribui√ß√£o dos pedidos ao longo do tempo, vamos observar a distribui√ß√£o geogr√°fica.
A tabela 'customers' cont√©m o prefixo do CEP, cidade e estado de cada cliente.
Vale notar que existem dois identificadores: 'customer_id '(relacionado ao pedido) e 'customer_unique_id', que representa o cliente √∫nico.
"""

view_table('customers', 5)

"""## Quais s√£o as cidades com maior n√∫mero de pedidos no conjunto de dados?


Vamos criar uma query que retorna o top 10 dessas cidades. Para isso, faremos um JOIN entre as tabelas 'orders' e 'customers'.
"""

orders_per_city = """
SELECT
    customer_city AS customer_city,
    UPPER(customer_city) AS city,
    COUNT(orders.order_id) AS city_order_count
FROM
    customers
    JOIN orders USING (customer_id)
GROUP BY customer_city
ORDER BY city_order_count DESC
LIMIT 10
"""

pd.read_sql_query(orders_per_city, conn)

"""Vamos visualizar essas cidades em um gr√°fico de barras horizontais com o 'kind=barh'.
Como a fun√ß√£o barh do Matplotlib plota de baixo para cima, precisamos inverter a ordem dos resultados. Para isso, usaremos a query anterior como uma subquery no SQL.
"""

orders_per_city_reversed = f"""
SELECT *
FROM ({orders_per_city})
ORDER BY city_order_count
"""

top10_cidades = pd.read_sql_query(orders_per_city_reversed, conn)

plt.figure(figsize=(10, 6))
plt.barh(top10_cidades['city'], top10_cidades['city_order_count'])
plt.xlabel('N√∫mero de pedidos')
plt.ylabel('Cidade')
plt.title('Top 10 cidades por n√∫mero de pedidos')
plt.show()

"""Podemos ver que S√£o Paulo domina amplamente, com aproximadamente o dobro de pedidos em rela√ß√£o ao segundo lugar, Rio de Janeiro.
As demais cidades ‚Äî Belo Horizonte, Bras√≠lia, Curitiba e Campinas ‚Äî t√™m volumes consideravelmente menores, mas ainda expressivos.
O padr√£o indica forte concentra√ß√£o de pedidos nas grandes capitais e regi√µes metropolitanas, o que reflete maior densidade populacional e poder de compra

<br>

# Pre√ßo dos pedidos

- Qual √© o valor m√©dio de um pedido?
- A Olist lida com transa√ß√µes de itens de valor muito baixo ou muito alto?

vamos dar uma olhada na tabela order_items:
Ela cont√©m v√°rios IDs, al√©m do pre√ßo (em reais R$) e do custo de envio dos itens de cada pedido
"""

# Tabela order_items, primeiras 4 colunas
view_table('order_items', 5).iloc[:, :4]

# Tabela order_items, 2 √∫ltimas colunas
view_table('order_items', 5).iloc[:, 5:]

"""Qual a m√©dia do pre√ßo dos pedidos, considerando os custos de produtos e o frete pra envio? vamos analisar os pedidos com menores e maiores custos"""

order_price_stats = """
SELECT
    MIN(order_price) AS min_order_price,
    ROUND(AVG(order_price), 2) AS avg_order_price,
    MAX(order_price) AS max_order_price
FROM (
    SELECT
        orders.order_id,
        SUM(order_items.price + order_items.freight_value) AS order_price
    FROM orders
        JOIN order_items USING (order_id)
    GROUP BY orders.order_id
)
"""

pd.read_sql_query(order_price_stats, conn)

"""O pre√ßo m√©dio dos pedidos √© R$160,58

O pedido mais caro chega a R$13.664,08, quase 100x acima da m√©dia ‚Äî provavelmente uma distribui√ß√£o com cauda √† direita

Vamos ver a varia√ß√£o dos pre√ßos, separando o valor dos produtos e o frete.
"""

order_product_and_shipping_costs = """
SELECT
    orders.order_id,
    SUM(price) AS product_cost,
    SUM(freight_value) AS shipping_cost
FROM
    orders
    JOIN order_items USING (order_id)
WHERE order_status = 'delivered'
GROUP BY orders.order_id
"""

df = pd.read_sql_query(order_product_and_shipping_costs, conn)
df

"""Vamos plotar um histograma para cada tipo de custo.

Como os valores variam bastante, mas a maioria dos pedidos tem custo baixo, vamos limitar o eixo X para 500 reais (produtos) e 80 reais (frete), podendo ver a distribui√ß√£o dos valores mais comuns.
"""

plt.figure(figsize=(15, 6))
# Histograma para o custo total de produtos
plt.subplot(1, 2, 1)
plt.hist(df['product_cost'], bins=1000, color='blue')
plt.title('Custo do produto para pedidos < R$500')
plt.xlabel('Custo do produto (R$)')
plt.ylabel('Frequ√™ncia')
plt.xlim([0, 500])
# Histograma para o custo total do frete
plt.subplot(1, 2, 2)
plt.hist(df['shipping_cost'], bins=800, color='#ad865f')
plt.title('Custo do frete para pedidos < R$80')
plt.xlabel('Custo do produto (R$)')
plt.xlim([0, 80])
plt.show()

"""O valor dos produtos varia bastante, mas a maioria dos pedidos custa menos de R$ 200.

O frete geralmente fica entre 7 a 20 reais, podendo chegar a valores bem mais altos em alguns casos.

# Categorias de produtos

vamos observar a tabela 'products', que possui 9 colunas. Vamos analistar a categoria e o peso do produto
"""

view_table('products', 5).iloc[:, [0, 1, 5]]

query = """
SELECT DISTINCT product_category_name
FROM products
ORDER BY product_category_name;
"""

df = pd.read_sql_query(query, conn)
print(df)

"""S√£o 73 categorias √∫nicas de produtos. Vamos usar um treemap para mostrar as vendas relativas de cada uma. Como 71 √© muito, selecionaremos as 18 principais e agruparemos o resto em ‚ÄúOutras categorias‚Äù.

Primeiro, vamos calcular o total de vendas por categoria
"""

ranked_categories = """
SELECT
    product_category_name AS categoria,
    SUM(price) AS vendas,
    RANK() OVER (ORDER BY SUM(price) DESC) AS posicao
FROM order_items
    JOIN orders USING (order_id)
    JOIN products USING (product_id)
WHERE order_status = 'delivered'
GROUP BY product_category_name
"""

pd.read_sql_query(ranked_categories, conn)

category_sales_summary = f"""
WITH RankedCategories AS (
    {ranked_categories}
)
-- Top 18 categorias por vendas
SELECT
    categoria,
    vendas
FROM RankedCategories
WHERE posicao <= 18

UNION ALL

-- Outras categorias agrupadas
SELECT
    'Outras categorias' AS categoria,
    SUM(vendas) AS vendas
FROM RankedCategories
WHERE posicao > 18
"""

df = pd.read_sql_query(category_sales_summary, conn)
df

"""Vamos visualizar esses dados em um 'treemap' usando o a biblioteca 'Squarify':"""

!pip install squarify

import squarify

plt.figure(figsize=(15, 8))
plt.title('Vendas por categoria')
color = sns.color_palette("viridis", len(df))
squarify.plot(sizes=df['vendas'], label=df['categoria'],
              alpha=0.7, color=color, edgecolor="white", linewidth=2)
plt.axis('off')
plt.show()

"""Podemos entender melhor os tipos de produtos analisando a distribui√ß√£o do peso por categoria em boxplots.
Usaremos a lista das 18 principais categorias do dataframe anterior.
"""

top_18_categorias = tuple(categoria for categoria in df['categoria'] if categoria != 'Outras categorias')
print(top_18_categorias)

"""Como o SQLite n√£o possui fun√ß√£o de mediana, ordenaremos os produtos por categoria usando n√∫meros de linha e contaremos quantos produtos h√° em cada uma, salvando o resultado para gerar o gr√°fico."""

categorias_ordenadas = f"""
SELECT
    product_weight_g AS peso,
    product_category_name AS categoria,
    ROW_NUMBER() OVER(PARTITION BY product_category_name ORDER BY product_weight_g) AS categoria_linha,
    COUNT(*) OVER(PARTITION BY product_category_name) AS total_produtos
FROM products
JOIN order_items USING (product_id)
WHERE product_category_name IN {top_18_categorias}
"""

df = pd.read_sql_query(categorias_ordenadas, conn)
df

"""Podemos usar a consulta anterior 'categorias_ordenadas' para calcular a mediana de cada categoria e ordenar pelos valores medianos."""

categorias_por_mediana = f"""
WITH CategoriasOrdenadas AS (
    {categorias_ordenadas}
)
SELECT
    categoria,
    AVG(peso) AS mediana_peso
FROM CategoriasOrdenadas
WHERE
    -- N√∫mero √≠mpar de produtos: seleciona a linha do meio
    (total_produtos % 2 = 1 AND categoria_linha = (total_produtos + 1) / 2)
    OR
    -- N√∫mero par de produtos: seleciona as duas linhas centrais e calcula a m√©dia
    (total_produtos % 2 = 0 AND categoria_linha IN ((total_produtos / 2), (total_produtos / 2 + 1)))
GROUP BY categoria
ORDER BY mediana_peso;
"""

categorias_por_mediana_df = pd.read_sql_query(categorias_por_mediana, conn)
categorias_por_mediana_df

"""Agora bora plotar um gr√°fico com os boxplots."""

plt.figure(figsize=(12, 8))

# Define a ordem das categorias de acordo com a mediana calculada
ordem = categorias_por_mediana_df['categoria'].tolist()

# Cria o boxplot de peso por categoria
sns.boxplot(
    x='peso',
    y='categoria',
    data=df,
    order=ordem,
    showfliers=False
)

# Customiza√ß√£o dos eixos e t√≠tulo
plt.xlabel('Peso do produto (gramas)')
plt.ylabel('Categoria do produto')
plt.title('Distribui√ß√£o do peso dos produtos - Top 18 categorias por vendas')

# Limites e ticks do eixo X
plt.xlim(-100, 26100)
plt.xticks(ticks=range(0, 30000, 2500))
plt.yticks(fontsize=12)

plt.show()

"""Podemos ver no topo do gr√°fico que as categorias telefonia, inform√°tica_acess√≥rios, rel√≥gios_presentes e beleza_sa√∫de possuem itens leves (geralmente abaixo de 2 kg).
Na parte inferior, pcs e moveis_escrit√≥rio t√™m produtos bem mais pesados.

# Evolu√ß√£o mensal das vendas por categoria

Vamos analisar as tend√™ncias de vendas de algumas categorias selecionadas:
"""

selected_categories = ('beleza_saude',
    'automotivo',
    'brinquedos',
    'informatica_acessorios',
    'cama_mesa_banho')

"""Para criar um gr√°fico de linha com as vendas mensais de cada categoria, precisamos de uma matriz com a soma das vendas, onde as colunas s√£o as categorias e as linhas representam os meses."""

monthly_sales_selected_categories = f"""
SELECT
    strftime('%Y-%m', order_purchase_timestamp) AS ano_mes,
    SUM(CASE WHEN product_category_name = 'beleza_saude' THEN price END) AS beleza_saude,
    SUM(CASE WHEN product_category_name = 'automotivo' THEN price END) AS automotivo,
    SUM(CASE WHEN product_category_name = 'brinquedos' THEN price END) AS brinquedos,
    SUM(CASE WHEN product_category_name = 'informatica_acessorios' THEN price END) AS informatica_acessorios,
    SUM(CASE WHEN product_category_name = 'cama_mesa_banho' THEN price END) AS cama_mesa_banho
FROM orders
    JOIN order_items USING (order_id)
    JOIN products USING (product_id)
WHERE order_purchase_timestamp >= '2017-01-01'
    AND product_category_name IN {selected_categories}
GROUP BY ano_mes
"""

df = pd.read_sql_query(monthly_sales_selected_categories, conn)
df = df.set_index('ano_mes')
df

"""H√° poucos dados antes de 2017-01-01, ent√£o ignorei os pedidos feitos antes dessa data. Visualizando a s√©rie temporal:"""

# Converte o √≠ndice (ano_mes) para datetime
df.index = pd.to_datetime(df.index, format='%Y-%m')

# Cria figura e eixo
fig, ax = plt.subplots(figsize=(14, 8))

# Plota as linhas
df.plot(ax=ax, marker='o', linestyle='-')

# Ajusta os r√≥tulos do eixo X
ax.set_xticks(df.index)
ax.set_xticklabels(df.index.strftime('%Y-%m'), rotation=45, ha='right')

# T√≠tulos e r√≥tulos
plt.title('Evolu√ß√£o Mensal das Vendas por Categoria', fontsize=16)
plt.xlabel('Ano-M√™s', fontsize=12)
plt.ylabel('Vendas Mensais (R$)', fontsize=12)

# Legenda e grid
plt.legend(title='Categoria de Produto', title_fontsize=13, fontsize=11)
plt.grid(True, linestyle='--', alpha=0.5)

# Mostra o gr√°fico
plt.tight_layout()
plt.show()

"""Beleza e Sa√∫de lidera com crescimento constante e s√≥lido; Automotivo cresce de forma est√°vel e previs√≠vel.
Brinquedos tem pico forte no fim do ano, mostrando alta sazonalidade.
Inform√°tica sobe at√© 2018 e depois cai, exigindo revis√£o de oferta.
Cama, Mesa e Banho oscila bastante, com potencial em datas espec√≠ficas.

# Entrega dos pedidos

A tabela orders cont√©m v√°rios registros de tempo:

- order_purchase_timestamp ‚Äì quando o cliente faz o pedido.
- order_approved_at ‚Äì quando o pedido √© aprovado pela Olist.
- order_delivered_carrier_date ‚Äì quando √© enviado para a transportadora.
- order_delivered_customer_date ‚Äì quando o cliente recebe o pedido.
- order_estimated_delivery_date ‚Äì previs√£o de entrega.

Cada um marca uma etapa do processo de envio.
Agora vamos consultar os dados para visualizar o tempo m√©dio de cada etapa nas 10 cidades com mais pedidos.
"""

# Top 10 cidades com mais pedidos
top_cities_query = """
SELECT
    customer_city,
    COUNT(order_id) AS total_pedidos
FROM orders
JOIN customers USING (customer_id)
GROUP BY customer_city
ORDER BY total_pedidos DESC
LIMIT 10
"""

top_cities = pd.read_sql_query(top_cities_query, conn)
top_cities

order_stage_times_top_10_cities = f"""
SELECT
    UPPER(customer_city) AS city,
    AVG(JULIANDAY(order_approved_at) - JULIANDAY(order_purchase_timestamp)) AS approved,
    AVG(JULIANDAY(order_delivered_carrier_date) - JULIANDAY(order_approved_at)) AS delivered_to_carrier,
    AVG(JULIANDAY(order_delivered_customer_date) - JULIANDAY(order_delivered_carrier_date)) AS delivered_to_customer,
    AVG(JULIANDAY(order_estimated_delivery_date) - JULIANDAY(order_delivered_customer_date)) AS estimated_delivery
FROM orders
JOIN customers USING (customer_id)
WHERE customer_city IN {tuple(top_cities['customer_city'])}
GROUP BY city
ORDER BY approved + delivered_to_carrier + delivered_to_customer DESC
"""

df = pd.read_sql_query(order_stage_times_top_10_cities, conn)
df = df.set_index('city')
df

"""Bora ver em um gr√°fico de barra o nosso df"""

fig, ax = plt.subplots(figsize=(11, 7))
df.plot(kind='barh', stacked=True, color=['#c44f53', '#4c72b1', '#55a978', '#dd842275'], ax=ax)
ax.set_xlabel('M√©dia de dias')
ax.set_ylabel('Cidade')
fig.suptitle('M√©dia de dias em cada etapa do pedido (10 principais cidades por vendas)', fontsize=16, x=0.38, y=0.92)
ax.grid(True, linestyle='--', linewidth=0.5, axis='x')
max_bar_length = int(df.sum(axis=1).max())
ax.set_xticks(range(0, max_bar_length + 4))
ax.tick_params(axis='y', labelsize=14)
plt.legend(title='Etapa do pedido', title_fontsize=14, fontsize=14)
plt.show()

"""Vemos que o tempo entre a aprova√ß√£o e o envio ao transportador √© parecido em todas as cidades (aproximadamente 3 dias).
J√° o tempo at√© a entrega varia bastante: S√£o Paulo, Guarulhos e S√£o Bernardo levam cerca de 5 dias, enquanto Rio, Porto Alegre e Salvador demoram mais que o dobro.

As estimativas de entrega s√£o, em m√©dia, uma semana mais longas que o real, mostrando previs√µes conservadoras.

Agora, vamos analisar se h√° varia√ß√£o sazonal nos prazos de entrega:
"""

#vamos criar uma query para calcular o tempo m√©dio de entrega por DIA
daily_avg_shipping_time = """
SELECT
    -- Cpmvertemos o timestamp da compra em apenas data (sem hora)
    DATE(order_purchase_timestamp) AS purchase_date,
    -- Calculamos o tempo m√©dio (em dias) entre a compra e a entrega ao cliente
    AVG(JULIANDAY(order_delivered_customer_date) - JULIANDAY(order_purchase_timestamp))
        AS avg_delivery_time
FROM orders
-- Usamos apenas os pedidos dentro do per√≠odo de 1 ano (jun/2017 a jun/2018)
WHERE order_purchase_timestamp >= '2017-06-01' AND order_purchase_timestamp <= '2018-06-30'
-- Agrupamos por data de compra para obter a m√©dia di√°ria
GROUP BY DATE(order_purchase_timestamp)
"""

df = pd.read_sql_query(daily_avg_shipping_time, conn)
df

"""Plotando um gr√°fico de linha, conseguimos colocar uma linha em vermelho pra nos mostrar o tempo m√©dio global de entrega durante o per√≠odo."""

plt.figure(figsize=(15, 6))
plt.plot(pd.to_datetime(df['purchase_date']), df['avg_delivery_time'], label='M√©dia di√°ria', color='green')
plt.axhline(y=df['avg_delivery_time'].mean(), color='red', linestyle='--', label='M√©dia anual')
plt.ylabel('Dias')
plt.title('Tempo m√©dio de entrega (de Junho 2017 at√© Junho 2018)')
plt.gca().xaxis.set_major_locator(mdates.MonthLocator())
plt.xticks(rotation=45)
plt.legend()
plt.show()

"""Como podemos ver, h√° dois per√≠odos em que o tempo de entrega foi significativamente maior que a m√©dia: dezembro de 2017 e fevereiro‚Äìmar√ßo de 2018.

Os atrasos em dezembro provavelmente foram causados pelo movimento intenso das festas de fim de ano, enquanto os de fevereiro‚Äìmar√ßo podem ter sido resultado das [greves](https://agenciabrasil.ebc.com.br/geral/noticia/2018-03/contra-mudancas-no-plano-de-saude-trabalhadores-dos-correios-fazem-ato-no-rio) dos correios que ocorreram em alguns estados do Brasil nesse per√≠odo.

# Avalia√ß√µes de pedidos

O banco de dados possui uma tabela de avalia√ß√µes, onde os clientes atribuem notas de 1 a 5 e podem deixar coment√°rios. Vamos ver as colunas usadas na an√°lise.
"""

view_table('order_reviews', 5).iloc[:, [1, 2, 4]]

"""Vamos contar quantos pedidos tem para cada nota de aval:"""

review_score_count = """
SELECT
    review_score,
    COUNT(*) AS count
FROM order_reviews
GROUP BY review_score
"""
df = pd.read_sql_query(review_score_count, conn)
df

plt.figure(figsize=(10,6))
colors = ['#BC2023', '#EB442C', '#F8B324', '#6da814', '#0C6B37']
sns.barplot(x='review_score', y='count', data=df, hue='review_score', palette=colors, dodge=False)
plt.title('Distribui√ß√£o das notas')
plt.xlabel('Nota de avalia√ß√£o')
plt.ylabel('Quantidade')
plt.legend().remove()
plt.tight_layout()
plt.show()

"""Podemos observar que a maioria das avalia√ß√µes s√£o positivas, mas ainda h√° um n√∫mero consider√°vel de clientes insatisfeitos.

Para entender o motivo, vamos gerar uma nuvem de palavras com os coment√°rios que receberam nota 1 ou 2, unindo todas as mensagens com a fun√ß√£o GROUP_CONCAT do SQL
"""

comments_negativos = """
SELECT GROUP_CONCAT(review_comment_message, ' ') AS comments
FROM order_reviews
WHERE review_score IN (1,2)
"""

comments_negativos_df = pd.read_sql(comments_negativos, conn)['comments'][0]
comments_negativos_df[:150]

## gerando a nuvem de palavras
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(comments_negativos_df)
plt.figure(figsize=(15, 8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""A principal causa dos coment√°rios negativos est√° relacionada a atrasos na entrega, enquanto a insatisfa√ß√£o com a qualidade do produto ou erros no pedido aparentam ser fatores secund√°rios.

# Valor do tempo de vida do cliente

Valor do tempo de vida do cliente (LTV) √© uma m√©trica que estima o valor total que um cliente trar√° ao longo do relacionamento com a empresa. Vamos calcular o LTV usando a tabela 'order_payments', que mostra os pagamentos e m√©todos utilizados.
"""

view_table('order_payments', 5)

"""Para come√ßar, vamos calcular os componentes do LTV para cada cliente:

Frequ√™ncia de Compra (FC): n√∫mero de pedidos feitos pelo cliente.

Valor M√©dio do Pedido (VMP): total pago dividido pelo n√∫mero de pedidos.

Tempo M√©dio de Vida do Cliente (TMVC): n√∫mero de semanas entre o primeiro e o √∫ltimo pedido (m√≠nimo de 1).

Para deixar o c√≥digo mais claro, usaremos uma tabela comum para reunir os dados necess√°rios e, em seguida, calcular cada componente do LTV na consulta principal.
"""

cte = """
-- Criamos uma CTE (subquery nomeada) chamada CustomerData
-- Ela re√∫ne os dados principais de cada cliente (pedidos, pagamentos e datas)
WITH CustomerData AS (
    SELECT
        customer_unique_id,
        customer_zip_code_prefix AS zip_code_prefix,
        COUNT(DISTINCT orders.order_id) AS order_count,
        SUM(payment_value) AS total_payment,
        JULIANDAY(MIN(order_purchase_timestamp)) AS first_order_day,
        JULIANDAY(MAX(order_purchase_timestamp)) AS last_order_day
    FROM customers
        JOIN orders USING (customer_id)
        JOIN order_payments USING (order_id)
    GROUP BY customer_unique_id
)

-- A partir da CTE, calculamos os indicadores de valor do cliente (CLV)
SELECT
    customer_unique_id,
    zip_code_prefix,
    order_count AS FC,
    total_payment / order_count AS VMP,
    CASE
     -- Se a diferen√ßa entre o primeiro e √∫ltimo pedido for menor que 7 dias, define o tempo m√≠nimo como 1 semana
        WHEN (last_order_day - first_order_day) < 7 THEN 1
            -- Caso contr√°rio, calcula o tempo m√©dio de vida do cliente em semanas
        ELSE
            (last_order_day - first_order_day) / 7
        END AS TMVC -- Tempo m√©dio de vida do cliente
FROM CustomerData
"""

pd.read_sql(cte, conn)

"""Para criar um mapa da distribui√ß√£o do LTV pelo Brasil, tamb√©m precisamos adicionar a latitude e a longitude de cada prefixo do CEP, que podem ser encontradas na tabela 'geolocation'."""

view_table('geolocation', 5)

"""sabe-se que CLV = FC √ó VMP √ó TMVC
onde:

FC = Frequ√™ncia de compras (quantas vezes o cliente comprou)

VMP = Valor m√©dio por pedido

TMVC = Tempo m√©dio de vida do cliente
"""

clv_por_cep = f"""
WITH CustomerData AS (
    SELECT
        customer_unique_id,
        customer_zip_code_prefix AS zip_code_prefix,
        COUNT(DISTINCT orders.order_id) AS FP, -- Frequ√™ncia de compras
        SUM(payment_value) / COUNT(DISTINCT orders.order_id) AS VMP, -- Valor m√©dio por pedido
        CASE
            WHEN (JULIANDAY(MAX(order_purchase_timestamp)) - JULIANDAY(MIN(order_purchase_timestamp))) < 7 THEN 1
            ELSE (JULIANDAY(MAX(order_purchase_timestamp)) - JULIANDAY(MIN(order_purchase_timestamp))) / 7
        END AS TMVC -- Tempo m√©dio de vida do cliente (em semanas)
    FROM customers
        JOIN orders USING (customer_id)
        JOIN order_payments USING (order_id)
    GROUP BY customer_unique_id
),
CLVPorCEP AS (
    SELECT
        zip_code_prefix,
        AVG(FP * VMP * TMVC) AS CLV_medio,
        COUNT(*) AS numero_clientes
    FROM CustomerData
    GROUP BY zip_code_prefix
)
SELECT
    c.zip_code_prefix,
    c.CLV_medio,
    c.numero_clientes,
    g.geolocation_lat AS latitude,
    g.geolocation_lng AS longitude
FROM CLVPorCEP c
JOIN geolocation g
    ON c.zip_code_prefix = g.geolocation_zip_code_prefix
GROUP BY c.zip_code_prefix
"""

df = pd.read_sql(clv_por_cep, conn)
print(df.columns)

"""Podemos criar um mapa usando a biblioteca folium do python!
https://realpython.com/python-folium-web-maps-from-data/
"""

import folium
import numpy as np

mapa = folium.Map(location=[-14.2350, -51.9253], zoom_start=4)

for i, linha in df.iterrows():
    folium.CircleMarker(
        location=[linha['latitude'], linha['longitude']],
        radius=0.1 * np.sqrt(linha['numero_clientes']),
        color=None,
        fill_color='#85001d',
        fill_opacity=0.1 + 0.1 * np.sqrt(linha['CLV_medio'] / df['CLV_medio'].max()),
        popup=(
            f"<b>Prefixo CEP:</b> {int(linha['zip_code_prefix'])}<br>"
            f"<b>CLV M√©dio:</b> {linha['CLV_medio']:.2f}<br>"
            f"<b>Clientes:</b> {int(linha['numero_clientes'])}"
        )
    ).add_to(mapa)

mapa

"""Como podemos ver no mapa, a maior parte do valor dos clientes dos vendedores da Olist est√° concentrada no sudeste do Brasil, nas regi√µes mais populosas ‚Äî principalmente no estado de S√£o Paulo e nas √°reas pr√≥ximas √†s cidades do Rio de Janeiro e Belo Horizonte.

<br>

# Vendedores

vamos ver as primeiras linhas da tabela 'sellers'
"""

view_table('sellers', 5)

"""como os vendedores s√£o em sua maioria?
pequenos com poucas vendas ou grandes com alto volume? E como as avalia√ß√µes variam entre eles?
Podemos responder a isso com um gr√°fico de dispers√£o talvez, usando dados da tabela 'sellers'.
"""

# Consulta SQL para analisar a rela√ß√£o entre volume de vendas e pontua√ß√£o m√©dia de avalia√ß√µes dos vendedores
seller_review_scores_and_sales = '''
SELECT
    sellers.seller_id,  -- Identificador √∫nico de cada vendedor
    AVG(order_reviews.review_score) AS avg_review_score,  -- M√©dia das notas de avalia√ß√£o recebidas
    SUM(order_items.price) AS total_sales,                -- Soma total das vendas realizadas (receita bruta)
    COUNT(orders.order_id) AS num_orders                  -- Quantidade de pedidos feitos ao vendedor
FROM
    sellers
    -- Associa os itens de pedido para saber o que cada vendedor vendeu
    LEFT JOIN order_items ON sellers.seller_id = order_items.seller_id
    -- Conecta os pedidos para relacionar vendas e avalia√ß√µes
    LEFT JOIN orders ON order_items.order_id = orders.order_id
    -- Junta as avalia√ß√µes dos clientes aos pedidos
    LEFT JOIN order_reviews ON orders.order_id = order_reviews.order_id
GROUP BY
    sellers.seller_id  -- Agrupa as m√©tricas por vendedor
HAVING
    COUNT(orders.order_id) > 10  -- Mant√©m apenas vendedores com mais de 10 pedidos (filtra outliers e dados irrelevantes)
'''

df = pd.read_sql_query(seller_review_scores_and_sales, conn)
df

"""Podemos criar um gr√°fico de dispers√£o onde cada ponto representa um vendedor:
no eixo Y estar√° a m√©dia das avalia√ß√µes dos pedidos, no eixo X, o total de vendas, exibido em escala logar√≠tmica (j√° que a maioria vende pouco). Al√©m disso, a cor e o tamanho dos pontos indicar√£o a quantidade de pedidos de cada vendedor.
"""

plt.figure(figsize=(15, 8))
sns.scatterplot(data=df, x='total_sales', y='avg_review_score', size='num_orders', sizes=(10, 500),
                hue='num_orders', palette="crest", alpha=0.7)
plt.xscale('log')
plt.xlabel('Total de vendas (escala logar√≠tmica)')
plt.ylabel('M√©dia das notas de avalia√ß√£o')
plt.title('Vendedores: Notas de avalia√ß√£o x vendas')
plt.legend(title='N√∫mero de pedidos')
plt.show()

"""A maioria dos vendedores da Olist √© pequena, com poucas vendas.
Vendedores maiores t√™m avalia√ß√µes mais est√°veis, geralmente entre 3.5 e 4.5.
√Ä direita do gr√°fico, destaca-se um grande vendedor (colora√ß√£o mais roxa) com notas bem abaixo dos demais.

Tamb√©m tem casos de vendedores com o mesmo total de vendas, mas com diferentes n√∫meros de pedidos ‚Äî indicando valores m√©dios por pedido distintos.

Vamos tentar criar outro gr√°fico para entender melhor como os vendedores se distribuem pelo volume de pedidos.

Para isso, vamos agrup√°-los em 4 categorias com base na quantidade total de pedidos enviados:

- Grupo 1: 1 a 9 pedidos
- Grupo 2: 10 a 99 pedidos
- Grupo 3: 100 a 999 pedidos
- Grupo 4: 1000 ou mais pedidos
"""

vendedores_segmentados = """
SELECT
    seller_id,
    CASE
        WHEN COUNT(order_id) BETWEEN 1 AND 9 THEN '1-9 orders'
        WHEN COUNT(order_id) BETWEEN 10 AND 99 THEN '10-99 orders'
        WHEN COUNT(order_id) BETWEEN 100 AND 999 THEN '100-999 orders'
        ELSE '1000+ orders'
    END AS bucket
FROM order_items
GROUP BY seller_id
"""

pd.read_sql_query(vendedores_segmentados, conn).head(5)

vendedores_por_segmento = f"""
WITH VendedoresSegmentados AS (
    {vendedores_segmentados}
)
SELECT
    bucket,
    COUNT(seller_id) AS seller_count
FROM VendedoresSegmentados
GROUP BY bucket
"""

seller_buckets = pd.read_sql_query(vendedores_por_segmento, conn)
seller_buckets

"""Vamos visualizar o df."""

plt.figure(figsize=(12, 8))
sns.barplot(x='bucket', y='seller_count', data=seller_buckets, hue='bucket', palette="magma", dodge=False)
plt.title('N√∫mero de vendedores por pedidos (agrupados)')
plt.xlabel('Quantidade de pedidos por vendedor')
plt.ylabel('N√∫mero de vendedores')
plt.tight_layout()
plt.show()

"""Pode-se concluir que a maioria dos vendedores apresentam poucos pedidos (1 a 9 pedidos), e poucos vendedores ultrampassam 1000 pedidos, que √© um valor alto de pedidos.

- Os grandes vendedores t√™m tempos de envio menores?
- Para responder a essa pergunta, vamos tentar criar um gr√°fico com a distribui√ß√£o do tempo de envio para cada um dos quatro grupos anteriores.
- Vamos reutilizar a tabela anterior na consulta para obter os dados necess√°rios.
"""

tempos_envio_vendedores = f"""
-- Criamos uma CTE (tabela tempor√°ria) chamada 'VendedoresSegmentados'
-- que reutiliza a query anterior, onde os vendedores foram agrupados
-- em faixas (1-9 pedidos, 10-99, 100-999, 1000+)
WITH VendedoresSegmentados AS (
    {vendedores_segmentados}
)

-- Agora selecionamos os dados principais que queremos analisar

SELECT
    bucket,
    VendedoresSegmentados.seller_id,
    -- Calculamos o tempo de entrega em dias:
    -- diferen√ßa entre a data de entrega ao cliente e a data da compra
    JULIANDAY(order_delivered_customer_date) - JULIANDAY(order_purchase_timestamp) AS delivery_time
FROM orders
    -- Relacionamos a tabela de pedidos com os itens de pedido
    JOIN order_items USING (order_id)
    -- Ligamos tamb√©m √† tabela 'VendedoresSegmentados' para trazer o grupo de cada vendedor
    JOIN VendedoresSegmentados USING (seller_id)
WHERE order_status = 'delivered'
"""

df = pd.read_sql_query(tempos_envio_vendedores, conn)
df

"""Vamos criar quatro boxplots, correspondendo a cada grupo mostrado no gr√°fico anterior."""

plt.figure(figsize=(12, 8))

# Cria uma paleta de cores com base no n√∫mero de grupos de vendedores
palette = sns.color_palette('magma', len(seller_buckets['bucket'].unique()))

# Garante que a ordem dos grupos siga a ordem da vari√°vel 'bucket'
ordem_buckets = seller_buckets['bucket'].unique()

# Cria os boxplots com as cores e ajustes visuais
sns.boxplot(
    x='bucket',
    y='delivery_time',
    data=df,
    order=ordem_buckets,
    showfliers=False,   # remove outliers para um gr√°fico mais limpo
    hue='bucket',
    palette=palette,
    dodge=False
)

plt.title('Tempo de entrega por volume de pedidos do vendedor', fontsize=14)
plt.xlabel('Grupos de vendedores (por n√∫mero de pedidos)', fontsize=12)
plt.ylabel('Tempo de entrega (dias)', fontsize=12)
plt.legend().remove()
plt.show()

"""Por algum motivo, quanto maior o vendedor, mais demoradas tendem a ser as entregas.

Um an√°lise futura pode ser a seguinte:
- O que causa esse aumento no tempo de envio?
- Ser√° que os vendedores menores demoram menos entre a aprova√ß√£o do pedido e a entrega do pacote √† transportadora?

<br>

# Conclus√£o do projeto

Este projeto demonstrou como √© poss√≠vel conduzir uma an√°lise completa de e-commerce utilizando apenas SQL para manipula√ß√£o e explora√ß√£o dos dados, com Python aplicado √† visualiza√ß√£o e storytelling dos resultados. A abordagem evidenciou o poder das CTEs, joins e fun√ß√µes agregadas do SQL em conjunto com as bibliotecas pandas, matplotlib, seaborn e folium para transformar dados brutos em insights acion√°veis.


A an√°lise do dataset da Olist revelou padr√µes claros de comportamento e performance:

  üìà Crescimento constante nas vendas, com picos not√°veis em dezembro, impulsionados pelas compras de Natal.

  ‚è∞ Maior volume de pedidos entre 10h e 16h nos dias √∫teis, refor√ßando a import√¢ncia de campanhas matinais.

  üèôÔ∏è Concentra√ß√£o de vendas nas grandes capitais, especialmente S√£o Paulo e Rio de Janeiro, refletindo densidade populacional e poder de compra.

  üí∏ Pedidos com valor m√©dio em torno de R$160, predominando produtos leves e de ticket baixo.

  üß¥ Categorias como ‚ÄúBeleza e Sa√∫de‚Äù e ‚ÄúInform√°tica‚Äù lideram em volume de vendas e crescimento ao longo do tempo.

  üöö Diferen√ßas log√≠sticas significativas entre cidades, com prazos de entrega maiores fora do eixo Sudeste.

  ‚≠ê Predom√≠nio de avalia√ß√µes positivas, mas com recorrentes reclama√ß√µes sobre atrasos na entrega.

  üí∞ Valor do Tempo de Vida do Cliente (LTV) concentrados no Sudeste, indicando regi√µes priorit√°rias para reten√ß√£o e marketing.

  üßæ Maioria dos vendedores √© composta por pequenos lojistas, enquanto grandes vendedores, curiosamente, possuem prazos de entrega mais longos.

Al√©m dos resultados, o projeto refor√ßa minha capacidade de:

- Estruturar pipelines anal√≠ticos em SQL com CTEs reutiliz√°veis e l√≥gicas otimizadas;

- Traduzir consultas complexas em visualiza√ß√µes claras e interpret√°veis;

- Gerar insights estrat√©gicos orientados a neg√≥cio ‚Äî do comportamento de compra √† efici√™ncia operacional.

Em resumo, este estudo combinou racioc√≠nio anal√≠tico, dom√≠nio t√©cnico e storytelling de dados, transformando o banco da Olist em uma narrativa visual e pr√°tica ‚Äî exatamente o tipo de trabalho que une an√°lise, neg√≥cios e impacto real.
"""